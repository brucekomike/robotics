# 实验二 决策树
## 实验任务
- 基于空气质量监测数据，通过 Python 编程实现：
- 利用分类树建立空气质量等级的分类预测模型，对空气质量等级进行多分类预测；
- 利用回归树建立 PM2.5 浓度的回归预测模型，基于测试误差和训练误差确定合理的
树深度。
## 具体要求
- 按照下发的实验报告模板独立完成实验报告。
- 分类任务：
  - 基于 PM2.5、PM10、SO2、CO 和 NO2 五个特征，决策树生成算法可选
ID3 或 CART，
  - 需要输出训练精度和测试精度（训练精度不得低于 80%），输出分类
树的文本化表达结果和图形化结果。
- 回归任务：
  - 基于 CO 和 SO2 两个特征，分别建立树深度为 2~15 的回归树模型，基于
测试误差和测试误差确定合理的树深度，并绘制树深度与测试误差、测试误差的折
线图（要有图例）；
  - 绘制最优树深度下的回归树模型的回归平面，将预测值小于实
际值的点设为蓝色，其余为灰色。

## 报告材料

```text
实验名称：
基于决策树的多分类预测与回归预测
```
```text
实验目的：
理解决策树模型实现分类和回归任务的基本原理，能够在有限时
间内自主编写 Python 程序解决一个面向实际应用的基于决策树模型
的多分类预测与回归预测问题。
```
```text
实验设备：
运行 Sublime Text 3 代码编辑器和 Python 3.9.1 编译环境的计算机。
```



## **实验原理（或实验方案）：**

决策树是一种基本的分类与回归方法。它呈树形结构，可以看作是 if-then 规则的集合，具有可读性好、分类速度快的优点。决策树的学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。

### **1. 分类决策树**

分类决策树是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。

*   **特征选择**：其目的是选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。常用的特征选择标准有信息增益（ID3 算法）、信息增益率（C4.5 算法）和基尼指数（CART 算法）。
    *   **ID3 算法**：核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。
    *   **CART (Classification and Regression Tree) 算法**：既可用于分类也可用于回归。在分类任务中，CART 使用基尼指数最小化准则来进行特征选择，生成二叉树。基尼指数表示集合的不确定性，基尼指数越小，样本集合的纯度越高。
*   **决策树生成**：从根结点开始，对结点计算所有可能的特征的信息增益（或基尼指数等），选择最优特征，根据该特征的不同取值建立子结点，再对子结点递归地调用以上方法，构建决策树，直到所有训练数据子集基本被正确分类，或没有合适的特征为止。
*   **决策树剪枝**：生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有好的分类能力，即可能发生过拟合现象。剪枝是解决过拟合的有效手段，分为预剪枝和后剪枝。

### **2. 回归决策树**

回归决策树用于预测连续值。与分类树类似，回归树也是将特征空间划分为若干单元（即叶子节点），但在每个单元上的输出是一个连续值，通常是该单元内所有训练样本目标值的均值。

*   **CART 回归树生成**：CART 回归树采用平方误差最小化准则来选择最优划分特征和最优切分点。
    假设已将输入空间划分为 $M$ 个单元 $R_1, R_2, \dots, R_M$，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$。回归树模型可表示为 $f(x) = \sum_{m=1}^{M} c_m I(x \in R_m)$。
    当输入空间的划分确定时，可以用平方误差 $\sum_{x_i \in R_m} (y_i - c_m)^2$ 来表示训练误差，用平方误差最小的准则求解每个单元上的最优输出值，易知单元 $R_m$ 上的 $c_m$ 的最优值 $\hat{c}_m$ 是 $R_m$ 上所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值。
    选择最优切分变量 $j$ 和切分点 $s$ 的过程如下：
    遍历所有输入变量 $j$，对固定的切分变量 $j$ 扫描所有切分点 $s$，求解

    $$
    \min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \right]
    $$
    
    其中 $R_1(j,s) = \{x | x^{(j)} \le s\}$ 和 $R_2(j,s) = \{x | x^{(j)} > s\}$ 是划分后的两个区域。对每个区域找到最优的 $c_1$ 和 $c_2$（即区域内样本均值）。然后选择使上述表达式最小的 $(j,s)$ 对。
    递归地重复这个过程，直到满足停止条件（如节点中样本个数小于预定阈值，或平方误差减小量小于预定阈值，或达到预设的最大深度）。

## **实验步骤：**

### **一、数据准备与预处理**

1.  **加载数据**：使用 Pandas 库加载空气质量监测数据集。数据集中应包含 PM2.5、PM10、SO2、CO、NO2 等特征，以及用于分类任务的空气质量等级标签和用于回归任务的 PM2.5 浓度值。
2.  **数据清洗**：检查并处理缺失值（例如，删除包含缺失值的样本或使用均值/中位数填充）。检查并处理异常值（可选）。
3.  **数据转换**：确保所有特征列为数值类型。对于分类任务的目标变量（空气质量等级），如果其为文本类型，需要进行数值编码（如标签编码）。

### **二、分类任务：空气质量等级预测**

1.  **特征与目标选择**：选择 PM2.5、PM10、SO2、CO 和 NO2 作为特征变量 (X_cls)，空气质量等级作为目标变量 (y_cls)。
2.  **数据划分**：将数据集划分为训练集和测试集（例如，采用 80% 作为训练集，20% 作为测试集），确保划分的随机性。
3.  **模型训练**：
    *   选择决策树生成算法（ID3 或 CART）。在 Scikit-learn 中，`DecisionTreeClassifier` 可以通过设置 `criterion` 参数（如 `'gini'` 对应 CART，`'entropy'` 对应 ID3 思想）来实现。
    *   初始化分类决策树模型。可以设置一些参数以防止过拟合，如 `max_depth`（最大深度）、`min_samples_split`（节点最小分裂样本数）、`min_samples_leaf`（叶节点最小样本数）。
    *   使用训练集数据训练分类决策树模型。
4.  **模型评估**：
    *   使用训练好的模型对训练集进行预测，计算训练精度（Accuracy）。
    *   使用训练好的模型对测试集进行预测，计算测试精度。
    *   确保训练精度不低于 80%。
5.  **结果输出**：
    *   打印训练精度和测试精度。
    *   使用 `sklearn.tree.export_text` 函数输出分类树的文本化表达结果。
    *   使用 `sklearn.tree.export_graphviz` 函数结合 `graphviz` 库（或 `matplotlib`）输出分类树的图形化结果。

### **三、回归任务：PM2.5 浓度预测**

1.  **特征与目标选择**：选择 CO 和 SO2 作为特征变量 (X_reg)，PM2.5 浓度作为目标变量 (y_reg)。
2.  **数据划分**：将数据集划分为训练集和测试集（例如，采用 80% 作为训练集，20% 作为测试集）。
3.  **不同树深度的模型训练与评估**：
    *   初始化一个列表用于存储不同树深度下的训练误差和测试误差（例如，使用均方误差 MSE 作为评价指标）。
    *   循环设置树的深度从 2 到 15：
        *   初始化回归决策树模型 (`DecisionTreeRegressor`)，设置 `max_depth` 为当前循环的深度。
        *   使用训练集数据训练回归决策树模型。
        *   使用训练好的模型对训练集进行预测，计算训练误差 (MSE)。
        *   使用训练好的模型对测试集进行预测，计算测试误差 (MSE)。
        *   将当前深度的训练误差和测试误差存入列表。
4.  **确定合理的树深度**：
    *   使用 `matplotlib.pyplot` 绘制树深度与训练误差、测试误差的折线图。横轴为树深度，纵轴为误差。图中应包含图例，清晰标示训练误差曲线和测试误差曲线。
    *   观察折线图，选择一个测试误差较低且开始趋于平稳（或轻微上升，避免过拟合），同时训练误差和测试误差之间差距不过大的深度作为合理的树深度。
5.  **最优树深度下的回归平面绘制**：
    *   使用上一步确定的最优树深度，重新训练一个回归决策树模型。
    *   创建 CO 和 SO2 特征值范围内的网格数据 (meshgrid)。
    *   使用训练好的最优回归树模型对网格数据进行预测，得到预测的 PM2.5 浓度值，这将构成回归曲面。
    *   使用 `matplotlib.pyplot` 绘制 3D 回归平面图。
    *   在 3D 图上，同时绘制测试集的实际数据点。根据要求，将模型预测值小于实际值的点设为蓝色，其余点设为灰色。

## **实验数据：**

<!-- zh-hans: 此处应简要描述所使用的空气质量监测数据集的来源、样本量、特征概况等。由于您未提供具体数据，请在此处补充。 -->
例如：本实验使用的数据集为 XX 市空气质量监测数据，包含 XXXX 条记录。主要特征包括 PM2.5 (µg/m³), PM10 (µg/m³), SO2 (µg/m³), CO (mg/m³), NO2 (µg/m³)。目标变量为空气质量等级（优、良、轻度污染、中度污染、重度污染、严重污染）和 PM2.5 浓度 (µg/m³)。

## **实验结果与讨论：**

### **一、分类任务结果与讨论**

1.  **模型精度**：
    *   训练精度：[<!-- zh-hans: 此处填写实际训练精度，例如：0.85 (85%) -->]
    *   测试精度：[<!-- zh-hans: 此处填写实际测试精度，例如：0.82 (82%) -->]


2.  **分类树文本化表达**：


3.  **分类树图形化结果**：
`

### **二、回归任务结果与讨论**

1.  **树深度与误差关系**：

2.  **最优树深度下的回归平面**：
 
### **三、总结与展望**

通过本次实验，我理解了决策树模型在分类和回归任务中的基本原理和实现过程。
在分类任务中，学习了如何构建和评估分类决策树，并将其规则进行可视化。训练精度达到了预期目标。
在回归任务中，通过比较不同树深度下的训练误差和测试误差，确定了合理的树深度，并可视化了回归树的预测平面。
实验中遇到的问题可能包括：数据预处理的细节（如缺失值填充策略）、模型参数的选择（如最大深度、最小叶节点样本数）对结果的影响等。
决策树模型具有解释性强、易于理解的优点，但也容易过拟合。未来可以尝试决策树的剪枝算法（预剪枝和后剪枝）来进一步优化模型性能，或者学习集成学习方法如随机森林、梯度提升决策树（GBDT）等，它们通常能获得比单一决策树更好的预测效果。