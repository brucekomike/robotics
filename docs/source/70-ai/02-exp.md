# 实验二 决策树
## 实验任务
- 基于空气质量监测数据，通过Python编程实现：
- 利用分类树建立空气质量等级的分类预测模型，对空气质量等级进行多分类预测；
- 利用回归树建立PM2.5浓度的回归预测模型，基于测试误差和训练误差确定合理的
树深度。
## 具体要求
- 按照下发的实验报告模板独立完成实验报告。
- 分类任务：
  - 基于PM2.5、PM10、SO2、CO和NO2五个特征，决策树生成算法可选
ID3或CART，
  - 需要输出训练精度和测试精度（训练精度不得低于80%），输出分类
树的文本化表达结果和图形化结果。
- 回归任务：
  - 基于CO和SO2两个特征，分别建立树深度为2~15的回归树模型，基于
测试误差和测试误差确定合理的树深度，并绘制树深度与测试误差、测试误差的折
线图（要有图例）；
  - 绘制最优树深度下的回归树模型的回归平面，将预测值小于实
际值的点设为蓝色，其余为灰色。

## 报告材料

```text
实验名称：
基于决策树的多分类预测与回归预测
```
```text
实验目的：
理解决策树模型实现分类和回归任务的基本原理，能够在有限时
间内自主编写Python程序解决一个面向实际应用的基于决策树模型
的多分类预测与回归预测问题。
```
```text
实验设备：
运行Sublime Text 3代码编辑器和Python 3.9.1编译环境的计算机。
```



## **实验原理（或实验方案）：**

决策树是一种基本的分类与回归方法。它呈树形结构，可以看作是if-then规则的集合，具有可读性好、分类速度快的优点。决策树的学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。

### **1. 分类决策树**

分类决策树是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。

*   **特征选择**：其目的是选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。常用的特征选择标准有信息增益（ID3算法）、信息增益率（C4.5算法）和基尼指数（CART算法）。
    *   **ID3算法**：核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。
    *   **CART (Classification and Regression Tree) 算法**：既可用于分类也可用于回归。在分类任务中，CART使用基尼指数最小化准则来进行特征选择，生成二叉树。基尼指数表示集合的不确定性，基尼指数越小，样本集合的纯度越高。
*   **决策树生成**：从根结点开始，对结点计算所有可能的特征的信息增益（或基尼指数等），选择最优特征，根据该特征的不同取值建立子结点，再对子结点递归地调用以上方法，构建决策树，直到所有训练数据子集基本被正确分类，或没有合适的特征为止。
*   **决策树剪枝**：生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有好的分类能力，即可能发生过拟合现象。剪枝是解决过拟合的有效手段，分为预剪枝和后剪枝。

### **2. 回归决策树**

回归决策树用于预测连续值。与分类树类似，回归树也是将特征空间划分为若干单元（即叶子节点），但在每个单元上的输出是一个连续值，通常是该单元内所有训练样本目标值的均值。

*   **CART回归树生成**：CART回归树采用平方误差最小化准则来选择最优划分特征和最优切分点。
    假设已将输入空间划分为 $M$ 个单元 $R_1, R_2, \dots, R_M$，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$。回归树模型可表示为 $f(x) = \sum_{m=1}^{M} c_m I(x \in R_m)$。
    当输入空间的划分确定时，可以用平方误差 $\sum_{x_i \in R_m} (y_i - c_m)^2$ 来表示训练误差，用平方误差最小的准则求解每个单元上的最优输出值，易知单元 $R_m$ 上的 $c_m$ 的最优值 $\hat{c}_m$ 是 $R_m$ 上所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值。
    选择最优切分变量 $j$ 和切分点 $s$ 的过程如下：
    遍历所有输入变量 $j$，对固定的切分变量 $j$ 扫描所有切分点 $s$，求解

    $$
    \min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \right]
    $$
    
    其中 $R_1(j,s) = \{x | x^{(j)} \le s\}$ 和 $R_2(j,s) = \{x | x^{(j)} > s\}$ 是划分后的两个区域。对每个区域找到最优的 $c_1$ 和 $c_2$（即区域内样本均值）。然后选择使上述表达式最小的 $(j,s)$ 对。
    递归地重复这个过程，直到满足停止条件（如节点中样本个数小于预定阈值，或平方误差减小量小于预定阈值，或达到预设的最大深度）。

## **实验步骤：**

### **一、 数据准备与预处理**

1.  **加载数据**：使用Pandas库加载空气质量监测数据集。数据集中应包含PM2.5、PM10、SO2、CO、NO2等特征，以及用于分类任务的空气质量等级标签和用于回归任务的PM2.5浓度值。
2.  **数据清洗**：检查并处理缺失值（例如，删除包含缺失值的样本或使用均值/中位数填充）。检查并处理异常值（可选）。
3.  **数据转换**：确保所有特征列为数值类型。对于分类任务的目标变量（空气质量等级），如果其为文本类型，需要进行数值编码（如标签编码）。

### **二、 分类任务：空气质量等级预测**

1.  **特征与目标选择**：选择PM2.5、PM10、SO2、CO和NO2作为特征变量 (X_cls)，空气质量等级作为目标变量 (y_cls)。
2.  **数据划分**：将数据集划分为训练集和测试集（例如，采用80%作为训练集，20%作为测试集），确保划分的随机性。
3.  **模型训练**：
    *   选择决策树生成算法（ID3或CART）。在Scikit-learn中，`DecisionTreeClassifier` 可以通过设置 `criterion` 参数（如 `'gini'` 对应CART，`'entropy'` 对应ID3思想）来实现。
    *   初始化分类决策树模型。可以设置一些参数以防止过拟合，如 `max_depth`（最大深度）、`min_samples_split`（节点最小分裂样本数）、`min_samples_leaf`（叶节点最小样本数）。
    *   使用训练集数据训练分类决策树模型。
4.  **模型评估**：
    *   使用训练好的模型对训练集进行预测，计算训练精度（Accuracy）。
    *   使用训练好的模型对测试集进行预测，计算测试精度。
    *   确保训练精度不低于80%。
5.  **结果输出**：
    *   打印训练精度和测试精度。
    *   使用 `sklearn.tree.export_text` 函数输出分类树的文本化表达结果。
    *   使用 `sklearn.tree.export_graphviz` 函数结合 `graphviz` 库（或 `matplotlib`）输出分类树的图形化结果。

### **三、 回归任务：PM2.5浓度预测**

1.  **特征与目标选择**：选择CO和SO2作为特征变量 (X_reg)，PM2.5浓度作为目标变量 (y_reg)。
2.  **数据划分**：将数据集划分为训练集和测试集（例如，采用80%作为训练集，20%作为测试集）。
3.  **不同树深度的模型训练与评估**：
    *   初始化一个列表用于存储不同树深度下的训练误差和测试误差（例如，使用均方误差MSE作为评价指标）。
    *   循环设置树的深度从2到15：
        *   初始化回归决策树模型 (`DecisionTreeRegressor`)，设置 `max_depth` 为当前循环的深度。
        *   使用训练集数据训练回归决策树模型。
        *   使用训练好的模型对训练集进行预测，计算训练误差 (MSE)。
        *   使用训练好的模型对测试集进行预测，计算测试误差 (MSE)。
        *   将当前深度的训练误差和测试误差存入列表。
4.  **确定合理的树深度**：
    *   使用 `matplotlib.pyplot` 绘制树深度与训练误差、测试误差的折线图。横轴为树深度，纵轴为误差。图中应包含图例，清晰标示训练误差曲线和测试误差曲线。
    *   观察折线图，选择一个测试误差较低且开始趋于平稳（或轻微上升，避免过拟合），同时训练误差和测试误差之间差距不过大的深度作为合理的树深度。
5.  **最优树深度下的回归平面绘制**：
    *   使用上一步确定的最优树深度，重新训练一个回归决策树模型。
    *   创建CO和SO2特征值范围内的网格数据 (meshgrid)。
    *   使用训练好的最优回归树模型对网格数据进行预测，得到预测的PM2.5浓度值，这将构成回归曲面。
    *   使用 `matplotlib.pyplot` 绘制3D回归平面图。
    *   在3D图上，同时绘制测试集的实际数据点。根据要求，将模型预测值小于实际值的点设为蓝色，其余点设为灰色。

## **实验数据：**

<!-- zh-hans: 此处应简要描述所使用的空气质量监测数据集的来源、样本量、特征概况等。由于您未提供具体数据，请在此处补充。 -->
例如：本实验使用的数据集为XX市空气质量监测数据，包含XXXX条记录。主要特征包括PM2.5 (µg/m³), PM10 (µg/m³), SO2 (µg/m³), CO (mg/m³), NO2 (µg/m³)。目标变量为空气质量等级（优、良、轻度污染、中度污染、重度污染、严重污染）和PM2.5浓度 (µg/m³)。

## **实验结果与讨论：**

### **一、 分类任务结果与讨论**

1.  **模型精度**：
    *   训练精度：[<!-- zh-hans: 此处填写实际训练精度，例如：0.85 (85%) -->]
    *   测试精度：[<!-- zh-hans: 此处填写实际测试精度，例如：0.82 (82%) -->]

    ```{note}
    <!-- zh-hans: 讨论：训练精度达到了80%以上的要求。训练精度与测试精度之间的差异较小/较大，说明模型泛化能力良好/存在一定的过拟合或欠拟合。如果存在问题，可以尝试调整树的参数如`max_depth`、`min_samples_leaf`或进行剪枝操作。 -->
    ```

2.  **分类树文本化表达**：
    <!-- zh-hans: 此处粘贴 `export_text` 输出的部分关键规则，并进行简要解读 -->
    例如：
    ```text
    |--- feature_0 (PM2.5) <= 75.50
    |   |--- feature_2 (SO2) <= 15.00
    |   |   |--- class: 0 (优)
    |   |--- feature_2 (SO2) >  15.00
    |   |   |--- class: 1 (良)
    |--- feature_0 (PM2.5) >  75.50
    |   |--- feature_4 (NO2) <= 50.00
    |   |   |--- class: 2 (轻度污染)
    ...
    ```
    ```{note}
    <!-- zh-hans: 讨论：从文本化结果可以看出决策树是如何根据PM2.5、SO2、NO2等特征的阈值进行分类决策的。例如，当PM2.5浓度小于等于75.50 µg/m³且SO2浓度小于等于15.00 µg/m³时，空气质量等级被预测为“优”。 -->
    ```

3.  **分类树图形化结果**：
    <!-- zh-hans: 此处应插入生成的决策树图。MyST 中可以使用 figure 指令，例如： -->
    <!--
    ````{figure} path/to/your/classification_tree_image.png
    ---
    name: fig-classification-tree
    align: center
    ---
    图1 空气质量等级分类决策树 <!-- zh-hans: (图表标题字体、大小等细节通常由最终渲染工具的主题或CSS控制) -->
    ````
    -->
    <div align="center">
      <!-- zh-hans: 请替换为实际图片 -->
      [占位符：图1 空气质量等级分类决策树]
      <p>图1 空气质量等级分类决策树 <!-- zh-hans: (图表标题字体、大小等细节通常由最终渲染工具的主题或CSS控制) --></p>
    </div>

    ```{note}
    <!-- zh-hans: 讨论：图形化结果直观地展示了决策树的结构。根节点是[例如：PM2.5]，说明该特征在分类中起到了最重要的初始划分作用。可以观察到树的深度、叶节点的纯度等信息。分析哪些特征对空气质量等级的判定影响较大。 -->
    ```

### **二、 回归任务结果与讨论**

1.  **树深度与误差关系**：
    <!-- zh-hans: 此处应插入树深度与训练误差、测试误差的折线图。 -->
    <!--
    ````{figure} path/to/your/depth_vs_error_image.png
    ---
    name: fig-depth-error
    align: center
    ---
    图2 树深度与PM2.5预测误差（MSE）的关系 <!-- zh-hans: (图表标题字体、大小等细节通常由最终渲染工具的主题或CSS控制) -->
    ````
    -->
    <div align="center">
      <!-- zh-hans: 请替换为实际图片 -->
      [占位符：图2 树深度与PM2.5预测误差（MSE）的关系]
      <p>图2 树深度与PM2.5预测误差（MSE）的关系 <!-- zh-hans: (图表标题字体、大小等细节通常由最终渲染工具的主题或CSS控制) --></p>
    </div>

    ```{note}
    <!-- zh-hans: 讨论：观察图2，训练误差随着树深度的增加通常会持续下降。测试误差则可能先下降后上升（或趋于平稳后略有波动）。当树深度较小时，模型可能欠拟合，训练误差和测试误差都较高。随着深度增加，模型拟合能力增强，测试误差下降。但当深度过大时，模型可能过拟合训练数据，导致测试误差反而上升。本实验选择的最优树深度为 [例如：5]，因为在此深度下，测试误差达到一个较低的水平，且与训练误差的差距在可接受范围内/测试误差开始有上升趋势。 -->
    ```

2.  **最优树深度下的回归平面**：
    选择的最优树深度为：[<!-- zh-hans: 例如：5 -->]
    <!-- zh-hans: 此处应插入最优树深度下，基于CO和SO2预测PM2.5的3D回归平面图。 -->
    <!--
    ````{figure} path/to/your/regression_plane_image.png
    ---
    name: fig-regression-plane
    align: center
    ---
    图3 最优树深度下的PM2.5浓度回归平面（CO, SO2为特征） <!-- zh-hans: (图表标题字体、大小等细节通常由最终渲染工具的主题或CSS控制) -->
    ````
    -->
    <div align="center">
      <!-- zh-hans: 请替换为实际图片 -->
      [占位符：图3 最优树深度下的PM2.5浓度回归平面（CO, SO2为特征）]
      <p>图3 最优树深度下的PM2.5浓度回归平面（CO, SO2为特征） <!-- zh-hans: (图表标题字体、大小等细节通常由最终渲染工具的主题或CSS控制) --></p>
    </div>

    ```{note}
    <!-- zh-hans: 讨论：图3展示了在最优树深度下，回归树模型学习到的CO、SO2与PM2.5浓度之间的关系。回归平面呈现阶梯状，这是回归树的典型特征，它将特征空间划分为若干矩形区域，每个区域内的预测值是恒定的。图中蓝色点表示模型预测值小于实际值的样本，灰色点表示预测值大于或等于实际值的样本。可以观察这些点的分布情况，评估模型在不同特征区域的预测偏差。例如，在CO和SO2浓度都较低的区域，模型预测可能偏高/偏低。回归树能够捕捉到特征与目标之间的非线性关系，但其预测结果不是连续平滑的。 -->
    ```

### **三、 总结与展望**

<!-- zh-hans: 总结本次实验学到的知识点，遇到的问题及解决方法，以及对决策树模型的进一步思考或改进方向。 -->
通过本次实验，我理解了决策树模型在分类和回归任务中的基本原理和实现过程。
在分类任务中，学习了如何构建和评估分类决策树，并将其规则进行可视化。训练精度达到了预期目标。
在回归任务中，通过比较不同树深度下的训练误差和测试误差，确定了合理的树深度，并可视化了回归树的预测平面。
实验中遇到的问题可能包括：数据预处理的细节（如缺失值填充策略）、模型参数的选择（如最大深度、最小叶节点样本数）对结果的影响等。
决策树模型具有解释性强、易于理解的优点，但也容易过拟合。未来可以尝试决策树的剪枝算法（预剪枝和后剪枝）来进一步优化模型性能，或者学习集成学习方法如随机森林、梯度提升决策树（GBDT）等，它们通常能获得比单一决策树更好的预测效果。